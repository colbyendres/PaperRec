# -*- coding: utf-8 -*-
"""CSCE670_FP_CLUSTER_AND_EMBEDDINGS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aG6HVBQccq69umFrBYHKuZDK7BioHIvs

# IMPORTS
"""

import os
import glob # Import the glob module
import tarfile
import gzip
import re
import pandas as pd
from google.colab import drive
from pathlib import Path
import re
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import pickle
import numpy as np

"""# PREREQS"""

from google.colab import drive
drive.mount('/content/drive')

root = Path("/content/drive/MyDrive/CSCE670 DATASET")
records = []

csv_path = '/content/drive/MyDrive/CSCE670 DATASET/new_df.csv'  # update to your path
df = pd.read_csv(csv_path)

print(df.shape)

mask = df['abstract'].notnull() & df['abstract'].str.strip().astype(bool)
new_d2f = df.loc[mask].copy()
csv_path = '/content/drive/MyDrive/CSCE670 DATASET/5000_papers.csv'
new_d2f.to_csv(csv_path, index=False)
print(new_d2f.shape)

"""finding avg number of words in papers

# CLUSTERING
"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from google.colab import drive
drive.mount('/content/drive')

csv_path = '/content/drive/MyDrive/CSCE670 DATASET/tex_papers2.csv'  # update to your path
df = pd.read_csv(csv_path)

print(df.shape)
df.head()

tokenizer = AutoTokenizer.from_pretrained("malteos/scincl")
model = AutoModel.from_pretrained("malteos/scincl").to(device)
model.eval()

embeddings = []
batch_size = 8
for i in range(0, len(df), batch_size):
    batch_texts = df["cleaned_full"].iloc[i : i + batch_size].tolist()
    inputs = tokenizer(
        batch_texts,
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    ).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
    hidden = outputs.last_hidden_state            # (B, L, D)
    mask = inputs.attention_mask.unsqueeze(-1)    # (B, L, 1)
    summed = (hidden * mask).sum(1)               # (B, D)
    counts = mask.sum(1)                          # (B, 1)
    batch_emb = (summed / counts).cpu().numpy()   # (B, D)
    embeddings.extend(batch_emb)

X = np.vstack(embeddings)
print("Embeddings matrix shape:", X.shape)

pca = PCA(n_components=0.80, random_state=42)
X_pca = pca.fit_transform(X)
print("PCA reduced to:", X_pca.shape)

# Choose k via Elbow & Silhouette
Ks = list(range(2, 11))
inertias, sil_scores = [], []

for k in Ks:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_pca)
    inertias.append(km.inertia_)
    sil_scores.append(silhouette_score(X_pca, labels))

plt.figure()
plt.plot(Ks, inertias)
plt.xlabel("k"); plt.ylabel("Inertia"); plt.title("Elbow Plot")
plt.show()

plt.figure()
plt.plot(Ks, sil_scores)
plt.xlabel("k"); plt.ylabel("Silhouette"); plt.title("Silhouette Scores")
plt.show()

opt_k = Ks[sil_scores.index(max(sil_scores))]
print("Optimal k:", opt_k)

n_clusters = 6
kmeans = KMeans(n_clusters=n_clusters, random_state=42)
df["cluster"] = kmeans.fit_predict(X_pca)
centroids = kmeans.cluster_centers_
print("Cluster sizes:\n", df["cluster"].value_counts())

with open("paper_pipeline.pkl2", "wb") as f:
    pickle.dump({"df": df, "embeddings": embeddings, "pca": pca, "kmeans": kmeans}, f)
print("Saved state to paper_pipeline.pkl")

"""# CHECKPOINT (PICKLE FILE EXISTS)"""

# IMPORTANT

STATE_PATH = "/content/paper_pipeline.pkl2"

if os.path.exists(STATE_PATH):
    with open(STATE_PATH, "rb") as f:
        state = pickle.load(f)
    df         = state["df"]
    embeddings = state["embeddings"]
    pca        = state["pca"]
    kmeans     = state["kmeans"]
    X = np.vstack(embeddings)
    X_pca = pca.fit_transform(X)
    print(f"Loaded df, embeddings, pca & kmeans from {STATE_PATH}")
else:
    raise FileNotFoundError(f"No saved state at {STATE_PATH}")

centroid_idxs = []

df["abstract"] = df["abstract"].fillna("")

for c, centroid in enumerate(kmeans.cluster_centers_):
    sims = cosine_similarity(centroid.reshape(1, -1), X_pca)[0]
    idx = sims.argmax()
    centroid_idxs.append(idx)

    row = df.iloc[idx]
    title    = row["title"]
    path     = row["file_path"]
    abstract = row["abstract"] or "No abstract available"
    snippet  = abstract[:200] + ("…" if len(abstract) > 200 else "")

    print(f"Cluster {c} centroid paper:")
    print(f" • Title   : {title}")
    print(f" • File    : {path}")
    print(f" • Abstract: {snippet}\n")

# Cluster 0 labels: Alexander Stratification, Fox Calculus and Alexander Invariants, Character Varieties of Finitely Presented Groups, Kähler Group Obstructions via Binomial Ideals, Betti Numbers of Finite Abelian Coverings

# Cluster 1 labels:

# Cluster 2 labels: Context-Dependent Entity Descriptions, Semantic Constraints in Lexical Choice, Supervised Learning of Linguistic Indicators, Language Reuse in Text Generation, Information Extraction–Driven Entity Profiling

# Cluster 3 labels: Super Shell Structure of Magnetic Susceptibility, Shell Structure in Confined Fermion Systems, Semiclassical Periodic Orbit Theory, Quantum Size Effects in Metal Clusters and Quantum Dots, Canonical Ensemble Effects on Mesoscopic Susceptibility

# Cluster 4 labels: Passive Tracer Dispersion in Closed Basins, Finite‑Size Lyapunov Exponents, Scale‑Dependent Diffusion Coefficients, Fixed‑Scale Mixing Analysis, Lagrangian Chaos in Bounded Flows

# Cluster 5 labels: d-wave superconductivity, twin boundaries, time-reversal symmetry breaking, Bogoliubov–de Gennes modeling, extended Hubbard interactions

def recommend(title, top_n=5):
    idxs = df.index[df["title"] == title].tolist()
    if not idxs:
        raise ValueError(f"Title '{title}' not found")
    idx = idxs[0]
    emb = embeddings[idx].reshape(1, -1)
    cluster_idxs = df.index[df["cluster"] == df.at[idx, "cluster"]].tolist()
    sims = cosine_similarity(emb, X[cluster_idxs])[0]
    ranked = sorted(zip(cluster_idxs, sims), key=lambda x: -x[1])
    return [(df.at[i, "title"], float(score)) for i, score in ranked if i != idx][:top_n]

print("Sample paper:", df.at[3012, "title"])
print("Top 5 recommendations:", recommend(df.at[3012, "title"]))

from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score,
    calinski_harabasz_score,
    davies_bouldin_score
)
from sklearn.metrics.pairwise import cosine_similarity

sil = silhouette_score(X_pca, df["cluster"])
ch  = calinski_harabasz_score(X_pca, df["cluster"])
db  = davies_bouldin_score(X_pca, df["cluster"])

print(f"\nSilhouette score     : {sil:.3f}")
print(f"Calinski–Harabasz     : {ch:.1f}")
print(f"Davies–Bouldin       : {db:.3f}\n")

"""Silhouette ≈ 0.483: you’re right on the border of “good” separation (≥ 0.5).

Calinski–Harabasz ≈ 6902.5: a very large jump—your between‑cluster variance is now huge relative to within‑cluster variance.

Davies–Bouldin ≈ 1.789: comfortably under 2, indicating your clusters are fairly compact and distinct.
"""

def get_cluster_for_title(title: str):
    """
    Look up a paper by its exact title and return its cluster label.
    """
    # find matching rows
    matches = df[df["title"] == title]
    if matches.empty:
        print(type(title))
        raise ValueError(f"No paper found with title '{title}'.")
    # grab the first match’s cluster
    cluster = int(matches["cluster"].iloc[0])
    #print(f"Paper '{title}' is assigned to cluster {cluster}.")
    return cluster

"""# USES LABELS GENERATED BY LABELEACHPAPER_GEMENI.py"""

df_labels = pd.read_csv('/content/drive/MyDrive/CSCE670 DATASET/all_labels.csv')
labels_with_cluster = df_labels.copy().dropna()
cluster_labels = []
i = 0
for title in labels_with_cluster['title']:
  title = str(title)
  cluster_labels.append(get_cluster_for_title(title))

labels_with_cluster['cluster'] = cluster_labels

labels_with_cluster.head()

from google.colab import files

import pandas as pd

labels_with_cluster.to_csv("labels_with_cluster.csv", index=False)

files.download("labels_with_cluster.csv")