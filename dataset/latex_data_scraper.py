# -*- coding: utf-8 -*-
"""LATEX DATA SCRAPER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DVAFtFXJ5FavskcNjMwvP71BGOnnx5hj
"""

!pip install arxiv

from google.colab import drive
drive.mount('/content/drive')

import os
import shutil

# Define a path in your Google Drive to save your files
drive_folder = '/content/drive/MyDrive/arxiv_pdf'
if not os.path.exists(drive_folder):
    os.makedirs(drive_folder)

!pip install arxiv==1.4.8

import arxiv
import tarfile

import os
import time
import tarfile
import gzip
import requests
import arxiv

# Base directory where both downloaded source files and their extracted contents will reside.
BASE_DIR = "arxiv_source"
os.makedirs(BASE_DIR, exist_ok=True)

# Overall target number of unique papers to process
total_target = 25000

# Batch size for processing results within each partition
batch_size = 2000

# Retry settings
partition_retry_attempts = 5   # Maximum retries for fetching a partition query
paper_retry_attempts = 1       # Maximum retries for downloading a paper

# Track how many papers we've processed and which paper IDs have been seen
downloaded_count = 0
unique_ids = set()

# Define date partitions as (start_date, end_date) tuples in the format YYYYMMDDHHMM.
# Adjust these ranges as needed to cover the desired time span.
partitions = [
    ("199001010000", "199412312359"),
    ("199501010000", "199912312359"),
    ("200001010000", "200412312359"),
    ("200501010000", "200912312359"),
    ("201001010000", "201412312359"),
    ("201501010000", "201912312359"),
    ("202001010000", "202512312359"),
]

def download_source(result, dest_dir):
    """
    Downloads the LaTeX source (e-print) file for an arXiv paper.
    """
    paper_id = result.get_short_id()
    source_url = f"https://arxiv.org/e-print/{paper_id}"
    response = requests.get(source_url, stream=True)
    if response.status_code != 200:
        raise Exception(f"Failed to fetch source for {paper_id}. Status code: {response.status_code}")

    filename = f"{paper_id}.tar.gz"
    if 'Content-Disposition' in response.headers:
        disp = response.headers['Content-Disposition']
        if "filename=" in disp:
            fname = disp.split("filename=")[1].strip(' "')
            filename = fname

    file_path = os.path.join(dest_dir, filename)
    with open(file_path, "wb") as f:
        for chunk in response.iter_content(chunk_size=1024):
            if chunk:
                f.write(chunk)
    return file_path

def extract_source(file_path, paper_id, base_dir):
    """
    Extracts the contents of the downloaded source file into a subfolder named after paper_id.
    - If the file is a valid tar archive (tar or tar.gz), it extracts all files.
    - Otherwise, if the file ends with .gz, it attempts to decompress it as a single file.
    Returns the path to the subfolder.
    """
    target_folder = os.path.join(base_dir, paper_id)
    os.makedirs(target_folder, exist_ok=True)

    if tarfile.is_tarfile(file_path):
        try:
            with tarfile.open(file_path, "r:*") as tar:
                tar.extractall(path=target_folder)
            print(f"Extracted tar archive contents to {target_folder}")
            return target_folder
        except Exception as e:
            print(f"Error extracting tar archive for {file_path}: {e}")

    if file_path.endswith(".gz"):
        try:
            with gzip.open(file_path, 'rb') as gz:
                content = gz.read()
            base_name = os.path.basename(file_path)
            output_filename = os.path.splitext(base_name)[0]
            output_path = os.path.join(target_folder, output_filename)
            with open(output_path, "wb") as f:
                f.write(content)
            print(f"Decompressed gz file to {output_path}")
        except Exception as e:
            print(f"Error decompressing gz file {file_path}: {e}")
    else:
        print(f"File {file_path} is not a valid tar archive and does not end with .gz. Skipping extraction.")

    return target_folder

def fetch_partition_results(query, max_results):
    """
    Creates an arxiv.Search object for the given query and returns a list of results.
    """
    search = arxiv.Search(
        query=query,
        max_results=max_results,
        sort_by=arxiv.SortCriterion.SubmittedDate,
        sort_order=arxiv.SortOrder.Ascending,
    )
    return list(search.results())

# Process each date partition
for start_date, end_date in partitions:
    if downloaded_count >= total_target:
        break
    query = f"cat:cs.* AND submittedDate:[{start_date} TO {end_date}]"
    print(f"\nProcessing partition: {start_date} to {end_date}")

    # Retry fetching the partition results if needed
    attempts = 0
    partition_results = []
    partition_max_results = 5000
    while attempts < partition_retry_attempts:
        try:
            partition_results = fetch_partition_results(query, partition_max_results)
            break
        except Exception as e:
            attempts += 1
            wait_time = 5 * (2 ** (attempts - 1))
            print(f"Error fetching results for partition {start_date} to {end_date}: {e}")
            print(f"Retrying in {wait_time} seconds (attempt {attempts}/{partition_retry_attempts})...")
            time.sleep(wait_time)
    if not partition_results:
        print(f"Skipping partition {start_date} to {end_date} after {partition_retry_attempts} attempts.")
        continue

    print(f"Fetched {len(partition_results)} results in partition.")

    # Process the results in batches
    for i in range(0, len(partition_results), batch_size):
        if downloaded_count >= total_target:
            break
        batch = partition_results[i:i+batch_size]
        for result in batch:
            if downloaded_count >= total_target:
                break
            paper_id = result.get_short_id()
            if paper_id in unique_ids:
                continue
            unique_ids.add(paper_id)
            print(f"Downloading source for {paper_id}: {result.title}")

            paper_attempts = 0
            success_download = False
            file_path = None
            while not success_download and paper_attempts < paper_retry_attempts:
                try:
                    file_path = download_source(result, BASE_DIR)
                    success_download = True
                    downloaded_count += 1
                except Exception as e:
                    paper_attempts += 1
                    wait_time = 5 * (2 ** (paper_attempts - 1))
                    print(f"Error downloading source for {paper_id} (attempt {paper_attempts}). Waiting {wait_time} seconds. Error: {e}")
                    time.sleep(wait_time)
            if not success_download:
                print(f"Skipping {paper_id} after {paper_retry_attempts} attempts.")
                continue


            if file_path:
                try:
                    extract_source(file_path, paper_id, BASE_DIR)
                except Exception as e:
                    print(f"Error extracting source for {paper_id}: {e}")
            time.sleep(1)
        print(f"Completed batch ending at index {i+batch_size} in partition. Total downloaded: {downloaded_count}")
        time.sleep(3)  # API DELAY

print(f"\nDownloaded and processed {downloaded_count} unique papers in total.")

# Option 1: Move your local folder to Google Drive
shutil.move("arxiv_source", drive_folder)

# Option 2: Or copy files over instead
# shutil.copytree("arxiv_pdf", drive_folder, dirs_exist_ok=True)